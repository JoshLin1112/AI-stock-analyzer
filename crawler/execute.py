import asyncio
import os
from datetime import datetime, timedelta
import pytz
import sys

import logging

# Add project root to the Python path to resolve module imports
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

logger = logging.getLogger("StockNewsCrawler")


from crawler.cnyes import CNYESCrawler
from crawler.eco import ECOCrawler
from crawler.ctee import CTEECrawler


class MultiCrawlerManager:
    def __init__(self, output_dir="./news"):
        tz = pytz.timezone("Asia/Taipei")
        now = datetime.now(tz)
        yesterday_14 = (now - timedelta(days=2)).replace(hour=14, minute=0, second=0, microsecond=0)

        self.start_time = yesterday_14
        self.end_time = now
        self.output_dir = output_dir

        self.crawlers = [
            CNYESCrawler(self.start_time, self.end_time, os.path.join(output_dir, "cnyes_news.csv")),
            ECOCrawler(self.start_time, self.end_time, os.path.join(output_dir, "eco_news.csv")),
            CTEECrawler(self.start_time, self.end_time, os.path.join(output_dir, "ctee_news.csv"), max_loads=2),
        ]

    async def run_all(self):
        """åŸ·è¡Œæ‰€æœ‰çˆ¬èŸ²"""
        logger.info("=== é–‹å§‹åŸ·è¡Œæ‰€æœ‰çˆ¬èŸ² ===")

        # å…ˆè·‘åŒæ­¥çš„å…©å€‹ (CNYES + ECO)
        for crawler in self.crawlers[:2]:
            # logger.info(f"ğŸš€ åŸ·è¡Œ {crawler.__class__.__name__}")
            crawler.run()

        # å†è·‘éåŒæ­¥çš„ CTEE
        # logger.info(f"ğŸš€ åŸ·è¡Œ {self.crawlers[2].__class__.__name__}")
        await self.crawlers[2].run()

        logger.info("=== æ‰€æœ‰çˆ¬èŸ²åŸ·è¡Œå®Œç•¢ ===")
        self.check_output_files()

    def check_output_files(self):
        """æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆæ˜¯å¦ç”Ÿæˆ"""
        logger.info("\n=== æª¢æŸ¥è¼¸å‡ºæª”æ¡ˆ ===")
        for crawler in self.crawlers:
            file_path = crawler.output_path
            if os.path.exists(file_path):
                size = os.path.getsize(file_path)
                logger.info(f"âœ“ {file_path} å­˜åœ¨ ({size} bytes)")
            else:
                logger.warning(f"âœ— {file_path} ä¸å­˜åœ¨")


if __name__ == "__main__":
    manager = MultiCrawlerManager()
    asyncio.run(manager.run_all())
