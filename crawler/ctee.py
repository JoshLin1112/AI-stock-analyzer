import asyncio
import os
import sys
from zoneinfo import ZoneInfo
from playwright.async_api import async_playwright
from datetime import datetime, timedelta
import pandas as pd

import logging

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

logger = logging.getLogger("StockNewsCrawler")

class CTEECrawler:
    def __init__(self, start_time: datetime, end_time: datetime, output_path: str, max_loads: int = 2, headless: bool = False):
        self.start_time = start_time
        self.end_time = end_time
        self.output_path = output_path
        self.max_loads = max_loads
        self.headless = headless
        self.browser = None
        self.context = None
        self.page = None
        self.seen_links = set()
        self.all_links = []
        self.results = []
        
    async def init_browser(self):
        """åˆå§‹åŒ–ç€è¦½å™¨"""
        self.playwright = await async_playwright().start()
        self.browser = await self.playwright.chromium.launch(
            headless=self.headless,
            args=[
                '--no-sandbox',
                '--disable-blink-features=AutomationControlled',
                '--disable-web-security',
                '--disable-features=VizDisplayCompositor'
            ]
        )
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        )
        self.page = await self.context.new_page()
        
    async def close_browser(self):
        """é—œé–‰ç€è¦½å™¨"""
        if self.browser:
            await self.browser.close()
        if hasattr(self, 'playwright'):
            await self.playwright.stop()
            
    # def parse_news_datetime(self, date_time_str):
    #     """è§£ææ–°èæ™‚é–“æ ¼å¼ , ex:'2025.08.22 14:00'"""
    #     try:
    #         return datetime.strptime(date_time_str.strip(), '%Y.%m.%d %H:%M')
    #     except ValueError as e:
    #         logger.warning(f"[è­¦å‘Š] æ™‚é–“æ ¼å¼è§£æå¤±æ•—: {date_time_str}, éŒ¯èª¤: {e}")
    #         return None
# from datetime import datetime
    from zoneinfo import ZoneInfo

    def parse_news_datetime(self, date_time_str):
        """è§£æå·¥å•†æ™‚å ±æ–°èçš„æ—¥æœŸæ ¼å¼ï¼Œæ”¯æ´å¤šç¨®æƒ…æ³"""
        from zoneinfo import ZoneInfo
        local_tz = ZoneInfo("Asia/Taipei")
        date_time_str = date_time_str.strip().replace("ã€€", "").replace("\xa0", "")  # ç§»é™¤å¥‡æ€ªç©ºç™½

        formats = [
            "%Y.%m.%d %H:%M",
            "%Y-%m-%d %H:%M",
            "%Y/%m/%d %H:%M",
            "%Y-%m-%d %H:%M:%S",
            "%Y.%m.%d %H:%M",
        ]

        for fmt in formats:
            try:
                dt = datetime.strptime(date_time_str, fmt)
                if fmt == "%Y.%m.%d":
                    dt = dt.replace(hour=0, minute=0)
                return dt.replace(tzinfo=local_tz)
            except ValueError:
                continue

        logger.warning(f"[è­¦å‘Š] æ™‚é–“æ ¼å¼è§£æå¤±æ•—: {date_time_str}")
        return None

    def should_skip_article(self, news_datetime):
        """åˆ¤æ–·æ–‡ç« æ™‚é–“æ˜¯å¦åœ¨æŒ‡å®šç¯„åœå…§"""
        if news_datetime is None:
            return True  # ç„¡æ³•è§£ææ™‚é–“ï¼Œè·³éè©²æ–‡ç« 
        return not (self.start_time <= news_datetime <= self.end_time)
        
    def should_stop_scraping(self, news_datetime):
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²åœæ­¢çˆ¬å–ï¼ˆæ™‚é–“æ—©æ–¼é–‹å§‹æ™‚é–“ï¼‰"""
        if news_datetime is None:
            return False
        return news_datetime < self.start_time
        
    async def scrape_current_page(self):
        """æŠ“å–ç•¶å‰é é¢çš„æ–°èé€£çµ"""
        cards = await self.page.locator('.newslist__card').all()
        for card in cards:
            try:
                title_el = card.locator('h3.news-title a')
                title = await title_el.inner_text()
                href = await title_el.get_attribute('href')
                date_el = card.locator('time.news-time')
                news_date = await date_el.inner_text()
                
                if href and href not in self.seen_links:
                    self.seen_links.add(href)
                    self.all_links.append({
                        "title": title.strip(),
                        "href": href,
                        "date": news_date.strip()
                    })
            except Exception as e:
                logger.error(f"[éŒ¯èª¤] æŠ“å–æ–°èå¤±æ•—: {e}")
                
    async def load_news_list(self):
        """è¼‰å…¥æ–°èåˆ—è¡¨é é¢"""
        logger.info("ğŸš€ åŸ·è¡Œå·¥å•†æ™‚å ±æ–°èçˆ¬èŸ²...")
        await self.page.goto("https://www.ctee.com.tw/stock/twmarket", timeout=60000)
        
        # ç¬¬ä¸€æ¬¡æŠ“å–
        await self.scrape_current_page()
        
        # æ¨¡æ“¬æ»‘å‹•ä¸¦é»æ“Šè¼‰å…¥æ›´å¤š
        for i in range(self.max_loads):
            try:
                await self.page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
                await self.page.wait_for_timeout(1500)
                
                load_more_btn = self.page.locator("button:has-text('è¼‰å…¥æ›´å¤š')")
                if await load_more_btn.is_visible():
                    await load_more_btn.click()
                    await self.page.wait_for_timeout(2000)  # ç­‰å¾…æ–°èè¼‰å…¥
                    await self.scrape_current_page()
                    logger.info(f"âœ… å·²å®Œæˆç¬¬ {i+1} æ¬¡è¼‰å…¥ï¼Œç´¯ç© {len(self.all_links)} å‰‡æ–°è")
                else:
                    logger.warning("âš ï¸ æ‰¾ä¸åˆ°ã€è¼‰å…¥æ›´å¤šã€æŒ‰éˆ•ï¼Œæå‰çµæŸ")
                    break
            except Exception as e:
                logger.error(f"[éŒ¯èª¤] é»æ“Šè¼‰å…¥æ›´å¤šå¤±æ•—: {e}")
                break
                
    async def scrape_article_content(self, item):
        """æŠ“å–å–®ç¯‡æ–‡ç« å…§å®¹"""
        link = item['href']
        full_url = link if link.startswith("http") else f"https://www.ctee.com.tw{link}"
        
        try:
            await self.page.goto(full_url, timeout=30000, wait_until="domcontentloaded")
            logger.info(f"âœ… å·²é€²å…¥ {full_url}")
            
            # æŠ“å–æ¨™é¡Œ
            title_el = await self.page.query_selector("h1.main-title")
            title = (await title_el.inner_text()).strip() if title_el else ""
            
            # æŠ“å–æ—¥æœŸå’Œæ™‚é–“
            await self.page.wait_for_selector("ul.news-credit li.publish-date time", timeout=5000)
            date_el = await self.page.query_selector("ul.news-credit li.publish-date time")
            time_el = await self.page.query_selector("ul.news-credit li.publish-time time")
            date_text = (await date_el.inner_text()).strip() if date_el else ""
            time_text = (await time_el.inner_text()).strip() if time_el else ""
            
            # çµ„åˆå®Œæ•´æ™‚é–“å­—ä¸²
            datetime_str = f"{date_text} {time_text}".strip()
            print(datetime_str)
            # æª¢æŸ¥æ™‚é–“æ˜¯å¦åœ¨æŒ‡å®šç¯„åœå…§
            news_datetime = self.parse_news_datetime(datetime_str)
            print(news_datetime)
            # å¦‚æœæ™‚é–“æ—©æ–¼é–‹å§‹æ™‚é–“ï¼Œåœæ­¢çˆ¬å–
            if self.should_stop_scraping(news_datetime):
                logger.info(f"â¹ï¸ æ–°èæ™‚é–“ {datetime_str} æ—©æ–¼é–‹å§‹æ™‚é–“ {self.start_time}ï¼Œåœæ­¢çˆ¬å–")
                return False
            
            # å¦‚æœæ™‚é–“ä¸åœ¨ç¯„åœå…§ï¼Œè·³éé€™ç¯‡æ–‡ç« 
            if self.should_skip_article(news_datetime):
                logger.info(f"â­ï¸ æ–°èæ™‚é–“ {datetime_str} ä¸åœ¨æŒ‡å®šç¯„åœå…§ï¼Œè·³éè©²æ–‡ç« ")
                return True
            
            # æŠ“å–å…§å®¹
            paragraphs = await self.page.query_selector_all("article p")
            content = "\n".join([
                (await p.inner_text()).strip() 
                for p in paragraphs 
                if (await p.inner_text()).strip()
            ])
            
            self.results.append({
                "time": datetime_str,
                "title": title,
                "content": content
            })
            
            return True  # è¿”å› True è¡¨ç¤ºç¹¼çºŒçˆ¬å–
            
        except Exception as e:
            logger.error(f"[éŒ¯èª¤] é€²å…¥å…§é å¤±æ•—: {e}")
            return True  # ç™¼ç”ŸéŒ¯èª¤ä½†ç¹¼çºŒçˆ¬å–ä¸‹ä¸€ç¯‡
            
    async def scrape_all_articles(self):
        """çˆ¬å–æ‰€æœ‰æ–‡ç« å…§å®¹"""
        logger.info(f"ğŸ“° é–‹å§‹çˆ¬å– {len(self.all_links)} å‰‡æ–°èå…§å®¹...")
        logger.info(f"â° æ™‚é–“ç¯„åœ: {self.start_time} è‡³ {self.end_time}")
        
        for i, item in enumerate(self.all_links, 1):
            logger.info(f"ğŸ“„ è™•ç†ç¬¬ {i}/{len(self.all_links)} ç¯‡æ–°è")
            
            # å¦‚æœè¿”å› Falseï¼Œè¡¨ç¤ºæ‡‰è©²åœæ­¢çˆ¬å–
            should_continue = await self.scrape_article_content(item)
            if not should_continue:
                logger.info(f"ğŸ›‘ å·²åœæ­¢çˆ¬å–ï¼Œå…±è™•ç†äº† {len(self.results)} ç¯‡æ–°è")
                break
                
    def save_to_csv(self):
        """å„²å­˜çµæœåˆ° CSV æª”æ¡ˆ"""
        if not self.results:
            logger.warning("âš ï¸ æ²’æœ‰æ•¸æ“šå¯ä¾›å„²å­˜")
            return
            
        df = pd.DataFrame(self.results)
        output_dir = os.path.dirname(self.output_path)
        
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)
            
        df.to_csv(self.output_path, index=False, encoding='utf-8-sig')
        logger.info(f"âœ… è³‡æ–™å„²å­˜è‡³ {self.output_path}")
        
    async def run(self):
        """åŸ·è¡Œå®Œæ•´çš„çˆ¬èŸ²æµç¨‹"""
        try:
            # logger.info("ğŸš€ é–‹å§‹åŸ·è¡Œæ–°èçˆ¬èŸ²...")
            
            # åˆå§‹åŒ–ç€è¦½å™¨
            await self.init_browser()
            
            # è¼‰å…¥æ–°èåˆ—è¡¨
            await self.load_news_list()
            logger.info(f"ğŸ“‹ å…±æ‰¾åˆ° {len(self.all_links)} å‰‡æ–°èé€£çµ")
            
            # çˆ¬å–æ–‡ç« å…§å®¹
            await self.scrape_all_articles()
            
            # å„²å­˜çµæœ
            self.save_to_csv()
            
        except Exception as e:
            logger.error(f"[åš´é‡éŒ¯èª¤] {e}")
        finally:
            await self.close_browser()
            logger.info("ğŸ”š çˆ¬èŸ²åŸ·è¡Œå®Œæˆ")


async def main():
    """ä¸»å‡½å¼"""
    # è¨­å®šæ™‚é–“ç¯„åœï¼ˆä¾‹å¦‚ï¼šæ˜¨å¤©14:00åˆ°ä»Šå¤©14:00ï¼‰
    start_time = datetime.now() - timedelta(days=1)
    start_time = start_time.replace(hour=14, minute=0, second=0, microsecond=0)
    
    end_time = datetime.now().replace(hour=14, minute=0, second=0, microsecond=0)
    
    output_path = "news/ctee_news.csv"
    max_loads = 2

    scraper = CTEECrawler(
        start_time=start_time,
        end_time=end_time,
        output_path=output_path,
        max_loads=max_loads,
        headless=False  # è¨­å®š headless=True å¯åœ¨èƒŒæ™¯åŸ·è¡Œ
    )
    await scraper.run()


if __name__ == "__main__":
    asyncio.run(main())