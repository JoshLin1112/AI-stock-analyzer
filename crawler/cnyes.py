import sys
import requests
import os
import re
from html import unescape
from datetime import datetime, timedelta, timezone
import pandas as pd

import logging

project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

logger = logging.getLogger("StockNewsCrawler")


class CNYESCrawler:
    def __init__(self, start_time: datetime, end_time: datetime, output_path: str, max_page: int = 3):
        """
        :param start_time: é–‹å§‹æ™‚é–“ (datetime)
        :param end_time: çµæŸæ™‚é–“ (datetime)
        :param output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘ (str)
        :param max_page: è¦æŠ“å–çš„æœ€å¤§é æ•¸ (int, é è¨­3é )
        """
        self.tz = timezone(timedelta(hours=8))  # å°ç£æ™‚å€

        # ç¢ºä¿æ™‚é–“è½‰æ›æˆå°ç£æ™‚å€å¾Œå†å– timestamp
        start_time = start_time.astimezone(self.tz)
        end_time = end_time.astimezone(self.tz)

        self.start_time_dt = start_time  # ä¿ç•™ datetime æ–¹ä¾¿å¾ŒçºŒéæ¿¾
        self.end_time_dt = end_time

        self.start_time = int(start_time.timestamp())
        self.end_time = int(end_time.timestamp())
        self.output_path = output_path
        self.max_page = max_page
        self.data = []

    def _fetch_page(self, page=1, limit=30, isCategoryHeadline=0):
        """æŠ“å–å–®é æ–°è"""
        url = "https://api.cnyes.com/media/api/v1/newslist/category/tw_stock"
        params = {
            "page": page,
            "limit": limit,
            "isCategoryHeadline": isCategoryHeadline,
            "startAt": self.start_time,
            "endAt": self.end_time
        }
        resp = requests.get(url, params=params)
        resp.raise_for_status()
        return resp.json()["items"]

    @staticmethod
    def _clean_html_content(text):
        """æ¸…ç† HTML æ¨™ç±¤å’Œäº‚ç¢¼"""
        if not text:
            return ""
        return re.sub(r'<[^>]+>|&[a-z0-9]+;|\s+', ' ', unescape(text)).strip()

    def crawl(self):
        """åŸ·è¡Œçˆ¬èŸ²ï¼Œå°‡çµæœå­˜å…¥ self.data"""
        logger.info("ğŸš€ åŸ·è¡Œé‰…äº¨ç¶²æ–°èçˆ¬èŸ²...")
        for page in range(1, self.max_page + 1):
            items = self._fetch_page(page=page, limit=30)
            logger.info(f"ç¸½æ–°èæ•¸: {items['total']} ç­†  æ¯é : {items['per_page']} ç­†  ç•¶å‰é : {items['current_page']} é ")

            for news in items["data"]:
                # è§£ææ–°èæ™‚é–“ â†’ å¼·åˆ¶ä½¿ç”¨å°ç£æ™‚å€
                published = datetime.fromtimestamp(news["publishAt"], tz=self.tz)
                content = self._clean_html_content(news["content"])

                self.data.append({
                    "time": published,
                    "title": news["title"],
                    "content": content
                })

    def save(self):
        """å„²å­˜çˆ¬å–çµæœåˆ° CSV (ä¸¦éæ¿¾æ™‚é–“å€é–“)"""
        if not self.data:
            logger.warning("âš ï¸ No data to save.")
            return

        df = pd.DataFrame(self.data)

        # éæ¿¾æ™‚é–“ï¼šåªä¿ç•™ [start_time_dt, end_time_dt]
        mask = (df["time"] >= self.start_time_dt) & (df["time"] <= self.end_time_dt)
        filtered_df = df.loc[mask].copy()

        # æ ¼å¼åŒ–æ™‚é–“æ¬„ä½
        filtered_df["time"] = filtered_df["time"].dt.strftime("%Y-%m-%d %H:%M:%S")

        output_dir = os.path.dirname(self.output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        filtered_df.to_csv(self.output_path, index=False, encoding='utf-8-sig')
        logger.info(f"âœ… è³‡æ–™å„²å­˜è‡³ {self.output_path}")

    def run(self):
        """å®Œæ•´æµç¨‹ï¼šçˆ¬èŸ² â†’ å­˜æª”"""
        self.crawl()
        self.save()


# --- æ¸¬è©¦ç”¨ ---
if __name__ == "__main__":
    now = datetime.now()
    yesterday_14 = (now - timedelta(days=2)).replace(hour=14, minute=0, second=0, microsecond=0)

    crawler = CNYESCrawler(
        start_time=yesterday_14,
        end_time=now,
        output_path="./news/cnyes_news.csv"
    )
    crawler.run()